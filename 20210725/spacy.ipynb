{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import spacy\r\n",
    "nlp = spacy.load(\"en_core_web_md\")\r\n",
    "doc = nlp(\"I went there\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "for token in doc:\r\n",
    "    print(token, type(token), token.text, type(token.text))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "I <class 'spacy.tokens.token.Token'> I <class 'str'>\n",
      "went <class 'spacy.tokens.token.Token'> went <class 'str'>\n",
      "there <class 'spacy.tokens.token.Token'> there <class 'str'>\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "doc = nlp(\"I own a pretty cat.\")\r\n",
    "\r\n",
    "print([token.text for token in doc], type([token.text for token in doc]))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['I', 'own', 'a', 'pretty', 'cat', '.'] <class 'list'>\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "doc = nlp(\"It's been a crazy week!!!\")\r\n",
    "\r\n",
    "print([token for token in doc])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[It, 's, been, a, crazy, week, !, !, !]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "from spacy.symbols import ORTH\r\n",
    "\r\n",
    "nlp = spacy.load(\"en_core_web_md\")\r\n",
    "doc = nlp(\"lemme that\")\r\n",
    "print([token.text for token in doc])\r\n",
    "\r\n",
    "special_case = [ {ORTH: \"lem\"}, {ORTH: \"me\"}]\r\n",
    "nlp.tokenizer.add_special_case(\"lemme\", special_case)\r\n",
    "print([token.text for token in doc])\r\n",
    "\r\n",
    "doc = nlp(\"Let's try again! Lemme that, lemme\")\r\n",
    "print([token.text for token in doc])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['lemme', 'that']\n",
      "['lemme', 'that']\n",
      "['Let', \"'s\", 'try', 'again', '!', 'Lemme', 'that', ',', 'lem', 'me']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "special_case1 = [ {ORTH: \"lem\"}, {ORTH:\"me\"}]\r\n",
    "special_case2 = [ {ORTH: \"Lem\"}, {ORTH:\"me\"}]\r\n",
    "nlp.tokenizer.add_special_case(\"lemme\", special_case1)\r\n",
    "nlp.tokenizer.add_special_case(\"Lemme\", special_case2)\r\n",
    "\r\n",
    "doc = nlp(\"Let's try again! Lemme that, lemme\")\r\n",
    "print([token.text for token in doc])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['Let', \"'s\", 'try', 'again', '!', 'Lem', 'me', 'that', ',', 'lem', 'me']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "\r\n",
    "special_case = [ {ORTH: \"...lemme...?\"}]\r\n",
    "nlp.tokenizer.add_special_case(\"...lemme...?\", special_case)\r\n",
    "\r\n",
    "\r\n",
    "doc = nlp(\"...lemme...?\")\r\n",
    "print([token.text for token in doc])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['...lemme...?']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")\r\n",
    "\r\n",
    "text = \"Let's go!\"\r\n",
    "doc = nlp(text)\r\n",
    "print([token.text for token in doc])\r\n",
    "\r\n",
    "detail_tokens = nlp.tokenizer.explain(text)\r\n",
    "for detail_token in detail_tokens:\r\n",
    "    print(detail_token, type(detail_token))\r\n",
    "    print(detail_token[1], \"\\t\", detail_token[0])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['Let', \"'s\", 'go', '!']\n",
      "('SPECIAL-1', 'Let') <class 'tuple'>\n",
      "Let \t SPECIAL-1\n",
      "('SPECIAL-2', \"'s\") <class 'tuple'>\n",
      "'s \t SPECIAL-2\n",
      "('TOKEN', 'go') <class 'tuple'>\n",
      "go \t TOKEN\n",
      "('SUFFIX', '!') <class 'tuple'>\n",
      "! \t SUFFIX\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "import spacy\r\n",
    "nlp = spacy.load(\"en_core_web_md\")\r\n",
    "\r\n",
    "text = \"I flied to N.Y yesterday. It was around 5 pm.\"\r\n",
    "doc = nlp(text)\r\n",
    "\r\n",
    "for sentence in doc.sents:\r\n",
    "    print(sentence)\r\n",
    "\r\n",
    "print([token.text for token in doc])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "I flied to N.Y yesterday.\n",
      "It was around 5 pm.\n",
      "['I', 'flied', 'to', 'N.Y', 'yesterday', '.', 'It', 'was', 'around', '5', 'pm', '.']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "text = \"According to the vaccine unit of SK Group, the trials will be conducted by comparing the candidate with AstraZeneca vaccines. In late May, the Ministry of Food and Drug Safety designed new comparative-style clinical trials, which compare the immunogenicity of an already authorized vaccine with a candidate under development in order to prove its efficacy. That was supposed to help local companies speed up development of Covid-19 vaccines without having to go through clinical trials that require large control groups.\"\r\n",
    "doc = nlp(text)\r\n",
    "\r\n",
    "for sentence in doc.sents:\r\n",
    "    print(sentence)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "According to the vaccine unit of SK Group, the trials will be conducted by comparing the candidate with AstraZeneca vaccines.\n",
      "In late May, the Ministry of Food and Drug Safety designed new comparative-style clinical trials, which compare the immunogenicity of an already authorized vaccine with a candidate under development in order to prove its efficacy.\n",
      "That was supposed to help local companies speed up development of Covid-19 vaccines without having to go through clinical trials that require large control groups.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "text = \"부산 해운대해수욕장에서 중학생 3명이 물놀이를 하던 중 1명이 실종되고 1명이 숨지는 사고가 발생했다. 25일 경찰과 소방당국에 따르면 이날 오전 3시 41분께 부산 해운대해수욕장에서 중학생 3명이 물놀이 하던 중 실종됐다는 신고가 접수됐다.\"\r\n",
    "doc = nlp(text)\r\n",
    "\r\n",
    "for sentence in doc.sents:\r\n",
    "    print(sentence)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "부산 해운대해수욕장에서 중학생 3명이 물놀이를 하던 중 1명이 실종되고 1명이 숨지는 사고가 발생했다.\n",
      "25일 경찰과 소방당국에 따르면 이날 오전 3시 41분께 부산 해운대해수욕장에서 중학생 3명이 물놀이 하던 중\n",
      "실종됐다는 신고가 접수됐다.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "import spacy\r\n",
    "nlp = spacy.load(\"en_core_web_md\")\r\n",
    "\r\n",
    "text = \"I went there for working and worked for 3 years.\"\r\n",
    "doc = nlp(text)\r\n",
    "\r\n",
    "for token in doc:\r\n",
    "    print(token.text, \"\\t\", token.lemma_)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "I \t I\n",
      "went \t go\n",
      "there \t there\n",
      "for \t for\n",
      "working \t working\n",
      "and \t and\n",
      "worked \t work\n",
      "for \t for\n",
      "3 \t 3\n",
      "years \t year\n",
      ". \t .\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "import spacy\r\n",
    "from spacy.symbols import ORTH, LEMMA\r\n",
    "nlp = spacy.load(\"en_core_web_md\")\r\n",
    "\r\n",
    "special_case = [ {ORTH: \"Angeltown\", LEMMA: \"Los Angeles\"}]\r\n",
    "nlp.tokenizer.add_special_case(\"Angeltown\", special_case)\r\n",
    "\r\n",
    "doc = nlp(\"I am flying to Angeltown\")\r\n",
    "for token in doc:\r\n",
    "    print(token.text, token.lemma_)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "[E1005] Unable to set attribute 'LEMMA' in tokenizer exception for 'Angeltown'. Tokenizer exceptions are only allowed to specify ORTH and NORM.",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14288/1460088413.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mspecial_case\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mORTH\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"Angeltown\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLEMMA\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"Los Angeles\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mnlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_special_case\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Angeltown\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspecial_case\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"I am flying to Angeltown\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\SONG\\Desktop\\SEOUL_AI\\20210725\\venv_20210725\\lib\\site-packages\\spacy\\tokenizer.pyx\u001b[0m in \u001b[0;36mspacy.tokenizer.Tokenizer.add_special_case\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\SONG\\Desktop\\SEOUL_AI\\20210725\\venv_20210725\\lib\\site-packages\\spacy\\tokenizer.pyx\u001b[0m in \u001b[0;36mspacy.tokenizer.Tokenizer._validate_special_case\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: [E1005] Unable to set attribute 'LEMMA' in tokenizer exception for 'Angeltown'. Tokenizer exceptions are only allowed to specify ORTH and NORM."
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "import spacy\r\n",
    "nlp = spacy.load(\"en_core_web_md\")\r\n",
    "\r\n",
    "doc = nlp(\"I know that you have been to Korea.\")\r\n",
    "\r\n",
    "for token in doc:\r\n",
    "    print(token)\r\n",
    "\r\n",
    "print(doc[2:4])\r\n",
    "print(doc[4:])\r\n",
    "print(doc[3:-1])\r\n",
    "print(doc[6:])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "I\n",
      "know\n",
      "that\n",
      "you\n",
      "have\n",
      "been\n",
      "to\n",
      "Korea\n",
      ".\n",
      "that you\n",
      "have been to Korea.\n",
      "you have been to Korea\n",
      "to Korea.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "import spacy\r\n",
    "nlp = spacy.load(\"en_core_web_md\")\r\n",
    "\r\n",
    "doc = nlp(\"I know that you have been to Korea.\")\r\n",
    "span = doc[2:4]\r\n",
    "for token in span:\r\n",
    "    print(token.text)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "that\n",
      "you\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "import spacy\r\n",
    "nlp = spacy.load(\"en_core_web_md\")\r\n",
    "\r\n",
    "doc = nlp(\"Hello, hi!\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "doc[0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Hello"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "doc[0].lower_"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'hello'"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "doc = nlp(\"HELLO, Hello, hello, hEllo\")\r\n",
    "for token in doc:\r\n",
    "    print(token. text)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "HELLO\n",
      ",\n",
      "Hello\n",
      ",\n",
      "hello\n",
      ",\n",
      "hEllo\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "print(doc[0].is_upper)\r\n",
    "print(doc[0].is_lower)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "doc = nlp(\"Cat and Cat123\")\r\n",
    "print(doc[0].is_alpha)\r\n",
    "print(doc[2].is_alpha)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "doc = nlp(\"ENglish and 한글!\")\r\n",
    "print(doc[0].is_ascii)\r\n",
    "print(doc[2].is_ascii)\r\n",
    "print(doc[3].is_ascii)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "True\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "doc = nlp(\"Cat Cat123 123\")\r\n",
    "print(doc[0].is_digit)\r\n",
    "print(doc[1].is_digit)\r\n",
    "print(doc[2].is_digit)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "False\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "doc = nlp(\"Hey, You and me!\")\r\n",
    "print(doc[0].is_punct)\r\n",
    "print(doc[1].is_punct)\r\n",
    "print(doc[2].is_punct)\r\n",
    "print(doc[3].is_punct)\r\n",
    "print(doc[4].is_punct)\r\n",
    "print(doc[5].is_punct)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "a = []\r\n",
    "\r\n",
    "for d in doc:\r\n",
    "    if d.is_punct:\r\n",
    "        a.append(d)\r\n",
    "    else:\r\n",
    "        pass\r\n",
    "\r\n",
    "print(a, len(a))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[,, !] 2\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "doc = nlp(\"( [ He said yes. ] )\")\r\n",
    "print(doc[0], doc[0].is_left_punct)\r\n",
    "print(doc[1], doc[1].is_left_punct)\r\n",
    "print(doc[-2], doc[-2].is_left_punct)\r\n",
    "print(doc[-1], doc[-1].is_left_punct)\r\n",
    "print(doc[-1], doc[-1].is_right_punct)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "( True\n",
      "[ True\n",
      "] False\n",
      ") False\n",
      ") True\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "source": [
    "doc = nlp(\" \")\r\n",
    "print(doc[0], len(doc[0]), doc[0].is_space)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  1 True\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "source": [
    "doc = nlp(\"   \")\r\n",
    "print(doc[0], len(doc[0]), doc[0].is_space)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "    3 True\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "source": [
    "doc = nlp(\"( you said [1] and {2} is not applicable.)\")\r\n",
    "print(doc[0], doc[0].is_bracket, doc[-1], doc[-1].is_bracket)\r\n",
    "print(doc[3], doc[3].is_bracket, doc[5], doc[5].is_bracket)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "( True ) True\n",
      "[ True ] True\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "source": [
    "doc = nlp(\"( you said '1\\\" is not applicable.)\")\r\n",
    "print(doc[3], doc[3].is_quote)\r\n",
    "print(doc[5], doc[5].is_quote)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "' True\n",
      "\" True\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "source": [
    "doc = nlp(\"I paid $12 for the shirt\")\r\n",
    "print(doc[2], doc[2].is_currency)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "$ True\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "source": [
    "doc = nlp(\"I emailed ou at least 100 times\")\r\n",
    "print(doc[-2], doc[-2].like_num)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "100 True\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "source": [
    "doc = nlp(\"I emailed ou at least hundred times\")\r\n",
    "print(doc[-2], doc[-2].like_num)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "hundred True\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "source": [
    "doc = nlp(\"My email is lilybad@naver.com and lilya!#b@naver.com you can visit me under http://www.naver.com any time you want\")\r\n",
    "\r\n",
    "print([token.text for token in doc])\r\n",
    "print(doc[3], doc[3].like_email)\r\n",
    "print(doc[5], doc[5].like_email)\r\n",
    "print(doc[-5], doc[-5].like_email)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['My', 'email', 'is', 'lilybad@naver.com', 'and', 'lilya!#b@naver.com', 'you', 'can', 'visit', 'me', 'under', 'http://www.naver.com', 'any', 'time', 'you', 'want']\n",
      "lilybad@naver.com True\n",
      "lilya!#b@naver.com False\n",
      "http://www.naver.com False\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "source": [
    "doc = nlp(\"Girl called Kathy has a nickname Cat123.\")\r\n",
    "for token in doc:\r\n",
    "    print(token.text, token.lemma_, token.shape_)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Girl girl Xxxx\n",
      "called call xxxx\n",
      "Kathy Kathy Xxxxx\n",
      "has have xxx\n",
      "a a x\n",
      "nickname nickname xxxx\n",
      "Cat123 cat123 Xxxddd\n",
      ". . .\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "source": [
    "doc = nlp(\"I visited Jenny at Korean Restorant Busan Myoinbong Seoungeun\")\r\n",
    "for token in doc:\r\n",
    "    print(token, token.is_oov)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "I False\n",
      "visited False\n",
      "Jenny False\n",
      "at False\n",
      "Korean False\n",
      "Restorant True\n",
      "Busan False\n",
      "Myoinbong True\n",
      "Seoungeun True\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "source": [
    "doc = nlp(\"I just want to inform you that I was with the principle.\")\r\n",
    "for token in doc:\r\n",
    "    print(token, token.is_stop)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "I True\n",
      "just True\n",
      "want False\n",
      "to True\n",
      "inform False\n",
      "you True\n",
      "that True\n",
      "I True\n",
      "was True\n",
      "with True\n",
      "the True\n",
      "principle False\n",
      ". False\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit ('venv_20210725': venv)"
  },
  "interpreter": {
   "hash": "c80c15c3a683148261a9a2ac122cb8b0741de3c3368a39506be719a091c476dd"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}