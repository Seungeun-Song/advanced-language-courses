{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import spacy\r\n",
    "nlp = spacy.load(\"en_core_web_md\")\r\n",
    "doc = nlp(\"I went there\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "for token in doc:\r\n",
    "    print(token, type(token), token.text, type(token.text))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "I <class 'spacy.tokens.token.Token'> I <class 'str'>\n",
      "went <class 'spacy.tokens.token.Token'> went <class 'str'>\n",
      "there <class 'spacy.tokens.token.Token'> there <class 'str'>\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "doc = nlp(\"I own a pretty cat.\")\r\n",
    "\r\n",
    "print([token.text for token in doc], type([token.text for token in doc]))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['I', 'own', 'a', 'pretty', 'cat', '.'] <class 'list'>\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "doc = nlp(\"It's been a crazy week!!!\")\r\n",
    "\r\n",
    "print([token for token in doc])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[It, 's, been, a, crazy, week, !, !, !]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "from spacy.symbols import ORTH\r\n",
    "\r\n",
    "nlp = spacy.load(\"en_core_web_md\")\r\n",
    "doc = nlp(\"lemme that\")\r\n",
    "print([token.text for token in doc])\r\n",
    "\r\n",
    "special_case = [ {ORTH: \"lem\"}, {ORTH: \"me\"}]\r\n",
    "nlp.tokenizer.add_special_case(\"lemme\", special_case)\r\n",
    "print([token.text for token in doc])\r\n",
    "\r\n",
    "doc = nlp(\"Let's try again! Lemme that, lemme\")\r\n",
    "print([token.text for token in doc])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['lemme', 'that']\n",
      "['lemme', 'that']\n",
      "['Let', \"'s\", 'try', 'again', '!', 'Lemme', 'that', ',', 'lem', 'me']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "special_case1 = [ {ORTH: \"lem\"}, {ORTH:\"me\"}]\r\n",
    "special_case2 = [ {ORTH: \"Lem\"}, {ORTH:\"me\"}]\r\n",
    "nlp.tokenizer.add_special_case(\"lemme\", special_case1)\r\n",
    "nlp.tokenizer.add_special_case(\"Lemme\", special_case2)\r\n",
    "\r\n",
    "doc = nlp(\"Let's try again! Lemme that, lemme\")\r\n",
    "print([token.text for token in doc])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['Let', \"'s\", 'try', 'again', '!', 'Lem', 'me', 'that', ',', 'lem', 'me']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "\r\n",
    "special_case = [ {ORTH: \"...lemme...?\"}]\r\n",
    "nlp.tokenizer.add_special_case(\"...lemme...?\", special_case)\r\n",
    "\r\n",
    "\r\n",
    "doc = nlp(\"...lemme...?\")\r\n",
    "print([token.text for token in doc])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['...lemme...?']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")\r\n",
    "\r\n",
    "text = \"Let's go!\"\r\n",
    "doc = nlp(text)\r\n",
    "print([token.text for token in doc])\r\n",
    "\r\n",
    "detail_tokens = nlp.tokenizer.explain(text)\r\n",
    "for detail_token in detail_tokens:\r\n",
    "    print(detail_token, type(detail_token))\r\n",
    "    print(detail_token[1], \"\\t\", detail_token[0])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['Let', \"'s\", 'go', '!']\n",
      "('SPECIAL-1', 'Let') <class 'tuple'>\n",
      "Let \t SPECIAL-1\n",
      "('SPECIAL-2', \"'s\") <class 'tuple'>\n",
      "'s \t SPECIAL-2\n",
      "('TOKEN', 'go') <class 'tuple'>\n",
      "go \t TOKEN\n",
      "('SUFFIX', '!') <class 'tuple'>\n",
      "! \t SUFFIX\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "import spacy\r\n",
    "nlp = spacy.load(\"en_core_web_md\")\r\n",
    "\r\n",
    "text = \"I flied to N.Y yesterday. It was around 5 pm.\"\r\n",
    "doc = nlp(text)\r\n",
    "\r\n",
    "for sentence in doc.sents:\r\n",
    "    print(sentence)\r\n",
    "\r\n",
    "print([token.text for token in doc])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "I flied to N.Y yesterday.\n",
      "It was around 5 pm.\n",
      "['I', 'flied', 'to', 'N.Y', 'yesterday', '.', 'It', 'was', 'around', '5', 'pm', '.']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "text = \"According to the vaccine unit of SK Group, the trials will be conducted by comparing the candidate with AstraZeneca vaccines. In late May, the Ministry of Food and Drug Safety designed new comparative-style clinical trials, which compare the immunogenicity of an already authorized vaccine with a candidate under development in order to prove its efficacy. That was supposed to help local companies speed up development of Covid-19 vaccines without having to go through clinical trials that require large control groups.\"\r\n",
    "doc = nlp(text)\r\n",
    "\r\n",
    "for sentence in doc.sents:\r\n",
    "    print(sentence)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "According to the vaccine unit of SK Group, the trials will be conducted by comparing the candidate with AstraZeneca vaccines.\n",
      "In late May, the Ministry of Food and Drug Safety designed new comparative-style clinical trials, which compare the immunogenicity of an already authorized vaccine with a candidate under development in order to prove its efficacy.\n",
      "That was supposed to help local companies speed up development of Covid-19 vaccines without having to go through clinical trials that require large control groups.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "text = \"부산 해운대해수욕장에서 중학생 3명이 물놀이를 하던 중 1명이 실종되고 1명이 숨지는 사고가 발생했다. 25일 경찰과 소방당국에 따르면 이날 오전 3시 41분께 부산 해운대해수욕장에서 중학생 3명이 물놀이 하던 중 실종됐다는 신고가 접수됐다.\"\r\n",
    "doc = nlp(text)\r\n",
    "\r\n",
    "for sentence in doc.sents:\r\n",
    "    print(sentence)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "부산 해운대해수욕장에서 중학생 3명이 물놀이를 하던 중 1명이 실종되고 1명이 숨지는 사고가 발생했다.\n",
      "25일 경찰과 소방당국에 따르면 이날 오전 3시 41분께 부산 해운대해수욕장에서 중학생 3명이 물놀이 하던 중\n",
      "실종됐다는 신고가 접수됐다.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "import spacy\r\n",
    "nlp = spacy.load(\"en_core_web_md\")\r\n",
    "\r\n",
    "text = \"I went there for working and worked for 3 years.\"\r\n",
    "doc = nlp(text)\r\n",
    "\r\n",
    "for token in doc:\r\n",
    "    print(token.text, \"\\t\", token.lemma_)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "I \t I\n",
      "went \t go\n",
      "there \t there\n",
      "for \t for\n",
      "working \t working\n",
      "and \t and\n",
      "worked \t work\n",
      "for \t for\n",
      "3 \t 3\n",
      "years \t year\n",
      ". \t .\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "import spacy\r\n",
    "from spacy.symbols import ORTH, LEMMA\r\n",
    "nlp = spacy.load(\"en_core_web_md\")\r\n",
    "\r\n",
    "special_case = [ {ORTH: \"Angeltown\", LEMMA: \"Los Angeles\"}]\r\n",
    "nlp.tokenizer.add_special_case(\"Angeltown\", special_case)\r\n",
    "\r\n",
    "doc = nlp(\"I am flying to Angeltown\")\r\n",
    "for token in doc:\r\n",
    "    print(token.text, token.lemma_)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "[E1005] Unable to set attribute 'LEMMA' in tokenizer exception for 'Angeltown'. Tokenizer exceptions are only allowed to specify ORTH and NORM.",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14288/1460088413.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mspecial_case\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mORTH\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"Angeltown\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLEMMA\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"Los Angeles\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mnlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_special_case\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Angeltown\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspecial_case\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"I am flying to Angeltown\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\SONG\\Desktop\\SEOUL_AI\\20210725\\venv_20210725\\lib\\site-packages\\spacy\\tokenizer.pyx\u001b[0m in \u001b[0;36mspacy.tokenizer.Tokenizer.add_special_case\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\SONG\\Desktop\\SEOUL_AI\\20210725\\venv_20210725\\lib\\site-packages\\spacy\\tokenizer.pyx\u001b[0m in \u001b[0;36mspacy.tokenizer.Tokenizer._validate_special_case\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: [E1005] Unable to set attribute 'LEMMA' in tokenizer exception for 'Angeltown'. Tokenizer exceptions are only allowed to specify ORTH and NORM."
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "import spacy\r\n",
    "nlp = spacy.load(\"en_core_web_md\")\r\n",
    "\r\n",
    "doc = nlp(\"I know that you have been to Korea.\")\r\n",
    "\r\n",
    "for token in doc:\r\n",
    "    print(token)\r\n",
    "\r\n",
    "print(doc[2:4])\r\n",
    "print(doc[4:])\r\n",
    "print(doc[3:-1])\r\n",
    "print(doc[6:])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "I\n",
      "know\n",
      "that\n",
      "you\n",
      "have\n",
      "been\n",
      "to\n",
      "Korea\n",
      ".\n",
      "that you\n",
      "have been to Korea.\n",
      "you have been to Korea\n",
      "to Korea.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "import spacy\r\n",
    "nlp = spacy.load(\"en_core_web_md\")\r\n",
    "\r\n",
    "doc = nlp(\"I know that you have been to Korea.\")\r\n",
    "span = doc[2:4]\r\n",
    "for token in span:\r\n",
    "    print(token.text)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "that\n",
      "you\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "import spacy\r\n",
    "nlp = spacy.load(\"en_core_web_md\")\r\n",
    "\r\n",
    "doc = nlp(\"Hello, hi!\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "doc[0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Hello"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "doc[0].lower_"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'hello'"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "doc = nlp(\"HELLO, Hello, hello, hEllo\")\r\n",
    "for token in doc:\r\n",
    "    print(token. text)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "HELLO\n",
      ",\n",
      "Hello\n",
      ",\n",
      "hello\n",
      ",\n",
      "hEllo\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "print(doc[0].is_upper)\r\n",
    "print(doc[0].is_lower)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "doc = nlp(\"Cat and Cat123\")\r\n",
    "print(doc[0].is_alpha)\r\n",
    "print(doc[2].is_alpha)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "doc = nlp(\"ENglish and 한글!\")\r\n",
    "print(doc[0].is_ascii)\r\n",
    "print(doc[2].is_ascii)\r\n",
    "print(doc[3].is_ascii)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "True\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "doc = nlp(\"Cat Cat123 123\")\r\n",
    "print(doc[0].is_digit)\r\n",
    "print(doc[1].is_digit)\r\n",
    "print(doc[2].is_digit)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "False\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "doc = nlp(\"Hey, You and me!\")\r\n",
    "print(doc[0].is_punct)\r\n",
    "print(doc[1].is_punct)\r\n",
    "print(doc[2].is_punct)\r\n",
    "print(doc[3].is_punct)\r\n",
    "print(doc[4].is_punct)\r\n",
    "print(doc[5].is_punct)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "a = []\r\n",
    "\r\n",
    "for d in doc:\r\n",
    "    if d.is_punct:\r\n",
    "        a.append(d)\r\n",
    "    else:\r\n",
    "        pass\r\n",
    "\r\n",
    "print(a, len(a))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[,, !] 2\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "doc = nlp(\"( [ He said yes. ] )\")\r\n",
    "print(doc[0], doc[0].is_left_punct)\r\n",
    "print(doc[1], doc[1].is_left_punct)\r\n",
    "print(doc[-2], doc[-2].is_left_punct)\r\n",
    "print(doc[-1], doc[-1].is_left_punct)\r\n",
    "print(doc[-1], doc[-1].is_right_punct)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "( True\n",
      "[ True\n",
      "] False\n",
      ") False\n",
      ") True\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "source": [
    "doc = nlp(\" \")\r\n",
    "print(doc[0], len(doc[0]), doc[0].is_space)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  1 True\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "source": [
    "doc = nlp(\"   \")\r\n",
    "print(doc[0], len(doc[0]), doc[0].is_space)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "    3 True\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "source": [
    "doc = nlp(\"( you said [1] and {2} is not applicable.)\")\r\n",
    "print(doc[0], doc[0].is_bracket, doc[-1], doc[-1].is_bracket)\r\n",
    "print(doc[3], doc[3].is_bracket, doc[5], doc[5].is_bracket)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "( True ) True\n",
      "[ True ] True\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "source": [
    "doc = nlp(\"( you said '1\\\" is not applicable.)\")\r\n",
    "print(doc[3], doc[3].is_quote)\r\n",
    "print(doc[5], doc[5].is_quote)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "' True\n",
      "\" True\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "source": [
    "doc = nlp(\"I paid $12 for the shirt\")\r\n",
    "print(doc[2], doc[2].is_currency)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "$ True\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "source": [
    "doc = nlp(\"I emailed ou at least 100 times\")\r\n",
    "print(doc[-2], doc[-2].like_num)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "100 True\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "source": [
    "doc = nlp(\"I emailed ou at least hundred times\")\r\n",
    "print(doc[-2], doc[-2].like_num)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "hundred True\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "source": [
    "doc = nlp(\"My email is lilybad@naver.com and lilya!#b@naver.com you can visit me under http://www.naver.com any time you want\")\r\n",
    "\r\n",
    "print([token.text for token in doc])\r\n",
    "print(doc[3], doc[3].like_email)\r\n",
    "print(doc[5], doc[5].like_email)\r\n",
    "print(doc[-5], doc[-5].like_email)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['My', 'email', 'is', 'lilybad@naver.com', 'and', 'lilya!#b@naver.com', 'you', 'can', 'visit', 'me', 'under', 'http://www.naver.com', 'any', 'time', 'you', 'want']\n",
      "lilybad@naver.com True\n",
      "lilya!#b@naver.com False\n",
      "http://www.naver.com False\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "source": [
    "doc = nlp(\"Girl called Kathy has a nickname Cat123.\")\r\n",
    "for token in doc:\r\n",
    "    print(token.text, token.lemma_, token.shape_)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Girl girl Xxxx\n",
      "called call xxxx\n",
      "Kathy Kathy Xxxxx\n",
      "has have xxx\n",
      "a a x\n",
      "nickname nickname xxxx\n",
      "Cat123 cat123 Xxxddd\n",
      ". . .\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "source": [
    "doc = nlp(\"I visited Jenny at Korean Restorant Busan Myoinbong Seoungeun\")\r\n",
    "for token in doc:\r\n",
    "    print(token, token.is_oov)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "I False\n",
      "visited False\n",
      "Jenny False\n",
      "at False\n",
      "Korean False\n",
      "Restorant True\n",
      "Busan False\n",
      "Myoinbong True\n",
      "Seoungeun True\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "source": [
    "doc = nlp(\"I just want to inform you that I was with the principle.\")\r\n",
    "for token in doc:\r\n",
    "    print(token, token.is_stop)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "I True\n",
      "just True\n",
      "want False\n",
      "to True\n",
      "inform False\n",
      "you True\n",
      "that True\n",
      "I True\n",
      "was True\n",
      "with True\n",
      "the True\n",
      "principle False\n",
      ". False\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import spacy\r\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "doc = nlp(\"Alicia and me went to the school by bus\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "for token in doc:\r\n",
    "    print(token.text, \"\\t\", token.pos_,\"\\t\", token.tag_, \"\\t\", spacy.explain(token.pos_), \"\\t\\t\\t\", spacy.explain(token.tag_))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Alicia \t PROPN \t NNP \t proper noun \t\t\t noun, proper singular\n",
      "and \t CCONJ \t CC \t coordinating conjunction \t\t\t conjunction, coordinating\n",
      "me \t PRON \t PRP \t pronoun \t\t\t pronoun, personal\n",
      "went \t VERB \t VBD \t verb \t\t\t verb, past tense\n",
      "to \t ADP \t IN \t adposition \t\t\t conjunction, subordinating or preposition\n",
      "the \t DET \t DT \t determiner \t\t\t determiner\n",
      "school \t NOUN \t NN \t noun \t\t\t noun, singular or mass\n",
      "by \t ADP \t IN \t adposition \t\t\t conjunction, subordinating or preposition\n",
      "bus \t NOUN \t NN \t noun \t\t\t noun, singular or mass\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "doc = nlp(\"My friend will fly to New York fast and she is staying there for 3 days.\")\r\n",
    "\r\n",
    "for token in doc:\r\n",
    "    print(token.text, \"\\t\", token.pos_,\"\\t\", token.tag_, \"\\t\", spacy.explain(token.pos_), \"\\t\\t\\t\", spacy.explain(token.tag_))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "My \t PRON \t PRP$ \t pronoun \t\t\t pronoun, possessive\n",
      "friend \t NOUN \t NN \t noun \t\t\t noun, singular or mass\n",
      "will \t AUX \t MD \t auxiliary \t\t\t verb, modal auxiliary\n",
      "fly \t VERB \t VB \t verb \t\t\t verb, base form\n",
      "to \t ADP \t IN \t adposition \t\t\t conjunction, subordinating or preposition\n",
      "New \t PROPN \t NNP \t proper noun \t\t\t noun, proper singular\n",
      "York \t PROPN \t NNP \t proper noun \t\t\t noun, proper singular\n",
      "fast \t ADV \t RB \t adverb \t\t\t adverb\n",
      "and \t CCONJ \t CC \t coordinating conjunction \t\t\t conjunction, coordinating\n",
      "she \t PRON \t PRP \t pronoun \t\t\t pronoun, personal\n",
      "is \t AUX \t VBZ \t auxiliary \t\t\t verb, 3rd person singular present\n",
      "staying \t VERB \t VBG \t verb \t\t\t verb, gerund or present participle\n",
      "there \t ADV \t RB \t adverb \t\t\t adverb\n",
      "for \t ADP \t IN \t adposition \t\t\t conjunction, subordinating or preposition\n",
      "3 \t NUM \t CD \t numeral \t\t\t cardinal number\n",
      "days \t NOUN \t NNS \t noun \t\t\t noun, plural\n",
      ". \t PUNCT \t . \t punctuation \t\t\t punctuation mark, sentence closer\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "doc = nlp(\"I will ship the package tomorrow. ---- I saw a red ship\")\r\n",
    "\r\n",
    "for token in doc:\r\n",
    "    print(token.text, \"\\t\", token.pos_,\"\\t\", token.tag_, \"\\t\", spacy.explain(token.pos_), \"\\t\\t\\t\", spacy.explain(token.tag_))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "I \t PRON \t PRP \t pronoun \t\t\t pronoun, personal\n",
      "will \t AUX \t MD \t auxiliary \t\t\t verb, modal auxiliary\n",
      "ship \t VERB \t VB \t verb \t\t\t verb, base form\n",
      "the \t DET \t DT \t determiner \t\t\t determiner\n",
      "package \t NOUN \t NN \t noun \t\t\t noun, singular or mass\n",
      "tomorrow \t NOUN \t NN \t noun \t\t\t noun, singular or mass\n",
      ". \t PUNCT \t . \t punctuation \t\t\t punctuation mark, sentence closer\n",
      "---- \t PUNCT \t NFP \t punctuation \t\t\t superfluous punctuation\n",
      "I \t PRON \t PRP \t pronoun \t\t\t pronoun, personal\n",
      "saw \t VERB \t VBD \t verb \t\t\t verb, past tense\n",
      "a \t DET \t DT \t determiner \t\t\t determiner\n",
      "red \t ADJ \t JJ \t adjective \t\t\t adjective (English), other noun-modifier (Chinese)\n",
      "ship \t NOUN \t NN \t noun \t\t\t noun, singular or mass\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "doc = nlp(\"My cat will fish for a fish tomorrow in a fishy way.\")\r\n",
    "\r\n",
    "for token in doc:\r\n",
    "    print(token.text, \"\\t\", token.pos_,\"\\t\", token.tag_, \"\\t\", spacy.explain(token.pos_), \"\\t\\t\\t\", spacy.explain(token.tag_))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "My \t PRON \t PRP$ \t pronoun \t\t\t pronoun, possessive\n",
      "cat \t NOUN \t NN \t noun \t\t\t noun, singular or mass\n",
      "will \t AUX \t MD \t auxiliary \t\t\t verb, modal auxiliary\n",
      "fish \t VERB \t VB \t verb \t\t\t verb, base form\n",
      "for \t ADP \t IN \t adposition \t\t\t conjunction, subordinating or preposition\n",
      "a \t DET \t DT \t determiner \t\t\t determiner\n",
      "fish \t NOUN \t NN \t noun \t\t\t noun, singular or mass\n",
      "tomorrow \t NOUN \t NN \t noun \t\t\t noun, singular or mass\n",
      "in \t ADP \t IN \t adposition \t\t\t conjunction, subordinating or preposition\n",
      "a \t DET \t DT \t determiner \t\t\t determiner\n",
      "fishy \t ADJ \t JJ \t adjective \t\t\t adjective (English), other noun-modifier (Chinese)\n",
      "way \t NOUN \t NN \t noun \t\t\t noun, singular or mass\n",
      ". \t PUNCT \t . \t punctuation \t\t\t punctuation mark, sentence closer\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "doc = nlp(\"고기를 잡으러 바다에 간다.\")\r\n",
    "\r\n",
    "for token in doc:\r\n",
    "    print(token.text, \"\\t\", token.pos_,\"\\t\", token.tag_, \"\\t\", spacy.explain(token.pos_), \"\\t\\t\\t\", spacy.explain(token.tag_))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "고기를 \t INTJ \t UH \t interjection \t\t\t interjection\n",
      "잡으러 \t PROPN \t NNP \t proper noun \t\t\t noun, proper singular\n",
      "바다에 \t PROPN \t NNP \t proper noun \t\t\t noun, proper singular\n",
      "간다 \t ADV \t RB \t adverb \t\t\t adverb\n",
      ". \t PUNCT \t . \t punctuation \t\t\t punctuation mark, sentence closer\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "sentence1 = \"I flew to Rome.\"\r\n",
    "sentence2 = \"I'm flying to Rome.\"\r\n",
    "sentence3 = \"I will fly to Rome.\"\r\n",
    "\r\n",
    "doc1 = nlp(sentence1)\r\n",
    "doc2 = nlp(sentence2)\r\n",
    "doc3 = nlp(sentence3)\r\n",
    "\r\n",
    "for doc in [doc1, doc2, doc3]:\r\n",
    "    print( [(w.text, w.lemma_) for w in doc if w.tag_ == \"VBG\" or w.tag_ == \"VB\"])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[]\n",
      "[('flying', 'fly')]\n",
      "[('fly', 'fly')]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "for doc in [doc1, doc2, doc3]:\r\n",
    "    for w in doc:\r\n",
    "        if w.tag_ ==\"VBG\" or w.tag_ ==\"VB\":\r\n",
    "            print([w.text, w.lemma_])\r\n",
    "        else:\r\n",
    "            print(\"None\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "['flying', 'fly']\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "['fly', 'fly']\n",
      "None\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "doc = nlp(\"He earned $5.5 million in 2020 and paid %35 max.\")\r\n",
    "\r\n",
    "for token in doc:\r\n",
    "    print(token.text, \"\\t\", token.tag_, \"\\t\",spacy.explain(token.tag_))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "He \t PRP \t pronoun, personal\n",
      "earned \t VBD \t verb, past tense\n",
      "$ \t $ \t symbol, currency\n",
      "5.5 \t CD \t cardinal number\n",
      "million \t CD \t cardinal number\n",
      "in \t IN \t conjunction, subordinating or preposition\n",
      "2020 \t CD \t cardinal number\n",
      "and \t CC \t conjunction, coordinating\n",
      "paid \t VBD \t verb, past tense\n",
      "% \t NN \t noun, singular or mass\n",
      "35 \t CD \t cardinal number\n",
      "max \t NN \t noun, singular or mass\n",
      ". \t . \t punctuation mark, sentence closer\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "doc = nlp(\"blue flower\")\r\n",
    "\r\n",
    "for token in doc:\r\n",
    "    print(token.text, token.dep_, spacy.explain(token.dep_))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "blue amod adjectival modifier\n",
      "flower ROOT None\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "doc = nlp(\"The president Donald Trump visited France.\")\r\n",
    "print(doc.ents)\r\n",
    "print(doc.ents[1])\r\n",
    "print(spacy.explain(doc[-2].ent_type_))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(Donald Trump, France)\n",
      "France\n",
      "Countries, cities, states\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "spacy.explain(\"ORG\")"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Companies, agencies, institutions, etc.'"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "doc = nlp(\"He worked for NASA\")\r\n",
    "print(doc.ents)\r\n",
    "print(doc[3])\r\n",
    "print(doc[3].ent_type_, \"//\", spacy.explain(doc[3].ent_type_))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(NASA,)\n",
      "NASA\n",
      "ORG // Companies, agencies, institutions, etc.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "doc = nlp(\"Albert Einstein was born in Ulm on 1879. He studied electronical engineering at ETH Zurich.\")\r\n",
    "doc.ents"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(Albert Einstein, Ulm, 1879, ETH Zurich)"
      ]
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "print(\"ents: \", doc.ents, \"\\n\")\r\n",
    "for token in doc:\r\n",
    "    print(token.text, \"\\t\", token.ent_type_, \"\\t\\t\\t\", spacy.explain(token.ent_type_))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ents:  (Albert Einstein, Ulm, 1879, ETH Zurich) \n",
      "\n",
      "Albert \t PERSON \t\t\t People, including fictional\n",
      "Einstein \t PERSON \t\t\t People, including fictional\n",
      "was \t  \t\t\t None\n",
      "born \t  \t\t\t None\n",
      "in \t  \t\t\t None\n",
      "Ulm \t GPE \t\t\t Countries, cities, states\n",
      "on \t  \t\t\t None\n",
      "1879 \t DATE \t\t\t Absolute or relative dates or periods\n",
      ". \t  \t\t\t None\n",
      "He \t  \t\t\t None\n",
      "studied \t  \t\t\t None\n",
      "electronical \t  \t\t\t None\n",
      "engineering \t  \t\t\t None\n",
      "at \t  \t\t\t None\n",
      "ETH \t ORG \t\t\t Companies, agencies, institutions, etc.\n",
      "Zurich \t ORG \t\t\t Companies, agencies, institutions, etc.\n",
      ". \t  \t\t\t None\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "doc = nlp(\"Jean-Michel Basquiat was an American artist of Haitian and Puerto Rican descent who gained fame with his graffiti and street art work\")\r\n",
    "\r\n",
    "print(\"ents: \", doc.ents, \"\\n\")\r\n",
    "for token in doc:\r\n",
    "    print(token.text, \"\\t\", token.ent_type_, \"\\t\\t\\t\\t\", spacy.explain(token.ent_type_))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ents:  (Jean-Michel Basquiat, American, Haitian, Puerto Rican) \n",
      "\n",
      "Jean \t PERSON \t\t\t\t People, including fictional\n",
      "- \t PERSON \t\t\t\t People, including fictional\n",
      "Michel \t PERSON \t\t\t\t People, including fictional\n",
      "Basquiat \t PERSON \t\t\t\t People, including fictional\n",
      "was \t  \t\t\t\t None\n",
      "an \t  \t\t\t\t None\n",
      "American \t NORP \t\t\t\t Nationalities or religious or political groups\n",
      "artist \t  \t\t\t\t None\n",
      "of \t  \t\t\t\t None\n",
      "Haitian \t NORP \t\t\t\t Nationalities or religious or political groups\n",
      "and \t  \t\t\t\t None\n",
      "Puerto \t NORP \t\t\t\t Nationalities or religious or political groups\n",
      "Rican \t NORP \t\t\t\t Nationalities or religious or political groups\n",
      "descent \t  \t\t\t\t None\n",
      "who \t  \t\t\t\t None\n",
      "gained \t  \t\t\t\t None\n",
      "fame \t  \t\t\t\t None\n",
      "with \t  \t\t\t\t None\n",
      "his \t  \t\t\t\t None\n",
      "graffiti \t  \t\t\t\t None\n",
      "and \t  \t\t\t\t None\n",
      "street \t  \t\t\t\t None\n",
      "art \t  \t\t\t\t None\n",
      "work \t  \t\t\t\t None\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "for ent in doc.ents:\r\n",
    "    print(ent, type(ent), ent.label_, spacy.explain(ent.label_))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Jean-Michel Basquiat <class 'spacy.tokens.span.Span'> PERSON People, including fictional\n",
      "American <class 'spacy.tokens.span.Span'> NORP Nationalities or religious or political groups\n",
      "Haitian <class 'spacy.tokens.span.Span'> NORP Nationalities or religious or political groups\n",
      "Puerto Rican <class 'spacy.tokens.span.Span'> NORP Nationalities or religious or political groups\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "doc = nlp(\"Saturn is the sixth planet from the Sun and the second-largest in the Solar System, after Jupiter. It is a gas giant with an average radius about nine times that of Earth.\")\r\n",
    "doc.ents"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(Saturn, sixth, Sun, second, the Solar System, Jupiter, about nine, Earth)"
      ]
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "for ent in doc.ents:\r\n",
    "    print(ent, \"\\t\", ent.label_,\"\\t\", spacy.explain(ent.label_))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Saturn \t PRODUCT \t Objects, vehicles, foods, etc. (not services)\n",
      "sixth \t ORDINAL \t \"first\", \"second\", etc.\n",
      "Sun \t PERSON \t People, including fictional\n",
      "second \t ORDINAL \t \"first\", \"second\", etc.\n",
      "the Solar System \t ORG \t Companies, agencies, institutions, etc.\n",
      "Jupiter \t LOC \t Non-GPE locations, mountain ranges, bodies of water\n",
      "about nine \t CARDINAL \t Numerals that do not fall under another type\n",
      "Earth \t LOC \t Non-GPE locations, mountain ranges, bodies of water\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import spacy\r\n",
    "from spacy import displacy\r\n",
    "\r\n",
    "nlp = spacy.load(\"en_core_web_md\")\r\n",
    "doc = nlp(\"Bill Gates is the CEO of Microsoft.\")\r\n",
    "displacy.render(doc, style=\"ent\")"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Bill Gates\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " is the CEO of \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Microsoft\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ".</div></span>"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import spacy\r\n",
    "from spacy import displacy\r\n",
    "\r\n",
    "nlp = spacy.load(\"en_core_web_md\")\r\n",
    "doc = nlp(\"Bill Gates is the CEO of Microsoft.\")\r\n",
    "displacy.render(doc, style=\"dep\")"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"4ae49a483d2945ef9353bd186860c694-0\" class=\"displacy\" width=\"1275\" height=\"312.0\" direction=\"ltr\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Bill</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">Gates</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">CEO</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">of</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">Microsoft.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-4ae49a483d2945ef9353bd186860c694-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,89.5 220.0,89.5 220.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-4ae49a483d2945ef9353bd186860c694-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-4ae49a483d2945ef9353bd186860c694-0-1\" stroke-width=\"2px\" d=\"M245,177.0 C245,89.5 395.0,89.5 395.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-4ae49a483d2945ef9353bd186860c694-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,179.0 L237,167.0 253,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-4ae49a483d2945ef9353bd186860c694-0-2\" stroke-width=\"2px\" d=\"M595,177.0 C595,89.5 745.0,89.5 745.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-4ae49a483d2945ef9353bd186860c694-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M595,179.0 L587,167.0 603,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-4ae49a483d2945ef9353bd186860c694-0-3\" stroke-width=\"2px\" d=\"M420,177.0 C420,2.0 750.0,2.0 750.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-4ae49a483d2945ef9353bd186860c694-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">attr</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M750.0,179.0 L758.0,167.0 742.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-4ae49a483d2945ef9353bd186860c694-0-4\" stroke-width=\"2px\" d=\"M770,177.0 C770,89.5 920.0,89.5 920.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-4ae49a483d2945ef9353bd186860c694-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M920.0,179.0 L928.0,167.0 912.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-4ae49a483d2945ef9353bd186860c694-0-5\" stroke-width=\"2px\" d=\"M945,177.0 C945,89.5 1095.0,89.5 1095.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-4ae49a483d2945ef9353bd186860c694-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1095.0,179.0 L1103.0,167.0 1087.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import spacy\r\n",
    "from spacy import displacy\r\n",
    "from pathlib import Path\r\n",
    "\r\n",
    "nlp = spacy.load(\"en_core_web_md\")\r\n",
    "doc = nlp(\"I am a butterfly.\")\r\n",
    "svg = displacy.render(doc, style=\"dep\", jupyter = False)\r\n",
    "filename = \"butterfly.svg\"\r\n",
    "output_path = Path(filename)\r\n",
    "output_path.open(\"w\", encoding=\"utf-8\").write(svg)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "3022"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "import spacy\r\n",
    "from spacy import displacy\r\n",
    "from pathlib import Path\r\n",
    "\r\n",
    "nlp = spacy.load(\"en_core_web_md\")\r\n",
    "doc = nlp(\"Bill Gates is the CEO of Microsoft.\")\r\n",
    "svg = displacy.render(doc, style=\"ent\", jupyter = False)\r\n",
    "filename = \"Bill_Gates.svg\"\r\n",
    "output_path = Path(filename)\r\n",
    "output_path.open(\"w\", encoding=\"utf-8\").write(svg)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "695"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit ('venv_20210725': venv)"
  },
  "interpreter": {
   "hash": "c80c15c3a683148261a9a2ac122cb8b0741de3c3368a39506be719a091c476dd"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}